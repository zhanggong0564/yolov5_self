{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "from models.yolo import get_model\n",
    "from config import config\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datasets import VOCDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(config)\n",
    "model_dict = torch.load('../log/Epoch_48.pt')['state_dict']\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_form_torch(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = image/255.0\n",
    "    image = np.expand_dims(np.transpose(image,[2,0,1]),0) \n",
    "    image = torch.from_numpy(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/mnt/e/AllData/VOC2012\"\n",
    "test_datasets = VOCDataset(True, 640, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_datasets,batch_size=4,collate_fn=test_datasets.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = torch.tensor([[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]]).view(3, 3, 2)\n",
    "for image,labels,visual_info in test_dataloader:\n",
    "    predicts = model(image)\n",
    "    b,_,_,_ = predicts[0].shape\n",
    "    objs=[]\n",
    "    for i,(predict,stride) in enumerate(zip(predicts,[8,16,32])):\n",
    "        b,c,h,w = predict.shape\n",
    "        predict = predict.view(b,3,5+20,h,w).permute(0, 1, 3, 4, 2).contiguous()\n",
    "        predict = predict.view(b, 3, -1, predict.size(-1)).sigmoid()\n",
    "        layer_confidence = predict[..., [4]] * predict[..., 5:]\n",
    "        keep_batch_indices, keep_anchor_indices, keep_cell_indices, object_classes = torch.where(layer_confidence>0.3)\n",
    "        object_score = layer_confidence[keep_batch_indices,keep_anchor_indices,keep_cell_indices,object_classes]\n",
    "\n",
    "        keepbox = predict[keep_batch_indices, keep_anchor_indices, keep_cell_indices].float()\n",
    "        layer_anchors = anchors[i]\n",
    "        keep_anchors = layer_anchors[keep_anchor_indices]\n",
    "        cell_x = keep_cell_indices % w\n",
    "        cell_y = keep_cell_indices // w\n",
    "        keep_cell_xy = torch.cat([cell_x.view(-1, 1), cell_y.view(-1, 1)], dim=1)\n",
    "        wh_restore = (torch.pow(keepbox[:, 2:4] * 2, 2) * keep_anchors) * stride\n",
    "        xy_restore = (keepbox[:, :2] * 2.0 - 0.5 + keep_cell_xy) * stride\n",
    "        object_score = object_score.float().view(-1, 1)\n",
    "        object_classes = object_classes.float().view(-1, 1)\n",
    "        keep_batch_indices = keep_batch_indices.float().view(-1, 1)\n",
    "        box = torch.cat((keep_batch_indices, xy_restore - (wh_restore - 1) * 0.5, xy_restore + (wh_restore - 1) * 0.5, object_score, object_classes), dim=1)\n",
    "        objs.append(box)\n",
    "        # if len(torch.where(layer_confidence > 0.1)[0]):\n",
    "        #     print(torch.where(layer_confidence > 0.1))\n",
    "    if len(objs) > 0:\n",
    "        objs_cat = torch.cat(objs, dim=0)\n",
    "        objs_image_base = []\n",
    "        for ibatch in range(b):\n",
    "            # left, top, right, bottom, score, classes\n",
    "            select_box = objs_cat[objs_cat[:, 0] == ibatch, 1:]\n",
    "            objs_image_base.append(select_box)\n",
    "    else:\n",
    "        objs_image_base = [torch.zeros((0, 6)) for _ in range(b)]\n",
    "    if 0.5 is not None:\n",
    "        # 使用类内的nms，类间不做操作\n",
    "        for ibatch in range(b):\n",
    "            image_objs = objs_image_base[ibatch]\n",
    "            if len(image_objs) > 0:\n",
    "                max_wh_size = 4096\n",
    "                classes = image_objs[:, [5]]\n",
    "                bboxes = image_objs[:, :4] + (classes * max_wh_size)\n",
    "                confidence = image_objs[:, 4]\n",
    "                keep_index = torchvision.ops.boxes.nms(bboxes, confidence, 0.5)\n",
    "                objs_image_base[ibatch] = image_objs[keep_index]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5599e+02, 2.7079e+02, 5.8655e+02, 6.4272e+02, 4.8398e-01, 1.4000e+01]],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs_image_base[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[272.83000946, 263.63788605, 267.99701691, 192.49513626,\n",
       "         13.        ],\n",
       "       [354.64916229, 255.21368027, 207.50047684, 207.36139297,\n",
       "         14.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_info[2]*[visual_info[1].shape[1],visual_info[1].shape[0],visual_info[1].shape[1],visual_info[1].shape[0],1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "from models.yolo import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    model_name  ='yolov5s'\n",
    "    num_classes = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint = torch.load('/home/zhanggong/Extern/workspace/yolo_serise/yolov5_self/yolov5s.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights =[]\n",
    "for k in checkpoint:\n",
    "    weights.append(checkpoint[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights =[]\n",
    "model_keys = []\n",
    "for k in model.state_dict():\n",
    "    # if 'running_mean' in k :continue\n",
    "    # if 'running_var' in k:continue\n",
    "    # if 'num_batches_tracked' in k:continue\n",
    "    model_weights.append(model.state_dict()[k])\n",
    "    model_keys.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "for i,weight in enumerate(weights):\n",
    "    new_dict[model_keys[i]] =  weight\n",
    "torch.save(new_dict,'yolov5_coco.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(weights)):\n",
    "    print(weights[i].shape==model_weights[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([255])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights[-1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "899241f4a75379447975d1af63d8c8aabee7d16326bb0b2b173a2507dd9ad7e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
